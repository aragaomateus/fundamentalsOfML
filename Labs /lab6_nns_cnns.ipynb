{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e976a81",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 7: Building Neural Networks using PyTorch - MLPs, ConvNets and Reccurent Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from plot_lib import set_default, plot_data, plot_model, set_default\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc1cae",
   "metadata": {},
   "source": [
    "## Classification of non-linearly separable dataset using multi-layer perceptron\n",
    "We will not demonstrate how to build a linear model and a neural network model using PyTorch and how to train it to classify a toy dataset which is not linearly separable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46927c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the hyperparameters for our network architecture\n",
    "seed = 12345\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 2  # dimensions\n",
    "C = 3  # num_classes\n",
    "H = 100  # num_hidden_units\n",
    "\n",
    "# Get our device in a variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d71aa4",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "We will now create a data set consisting of three classes and is in the shape of a spiral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros(N * C, D).to(device)\n",
    "y = torch.zeros(N * C, dtype=torch.long).to(device)\n",
    "for c in range(C):\n",
    "    index = 0\n",
    "    t = torch.linspace(0, 1, N)\n",
    "    # When c = 0 and t = 0: start of linspace\n",
    "    # When c = 0 and t = 1: end of linpace\n",
    "    # This inner_var is for the formula inside sin() and cos() like sin(inner_var) and cos(inner_Var)\n",
    "    inner_var = torch.linspace(\n",
    "        # When t = 0\n",
    "        (2 * math.pi / C) * (c),\n",
    "        # When t = 1\n",
    "        (2 * math.pi / C) * (2 + c),\n",
    "        N\n",
    "    ) + torch.randn(N) * 0.2\n",
    "    \n",
    "    for ix in range(N * c, N * (c + 1)):\n",
    "        X[ix] = t[index] * torch.FloatTensor((\n",
    "            math.sin(inner_var[index]), math.cos(inner_var[index])\n",
    "        ))\n",
    "        y[ix] = c\n",
    "        index += 1\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f7a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the data using the plot_data function provided as a helper function\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1686139",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "We now define a linear classification model using PyTorch and train it using stochastic gradient descent with the help of the autograd package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be412e0",
   "metadata": {},
   "source": [
    "Initialize some training hyper-parameter values, such as, learning rate, regularization coefficient etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b120b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "lambda_l2 = 1e-3 # coefficient for the L2 regularizer. You should play with its value to see the effect of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model. Notice the Sequential container class. \n",
    "# Each Linear module has a weight and bias\n",
    "# The order in which the Linear modules are defined is important as it creates the directed acyclic graph\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H,H),\n",
    "    nn.Linear(H, C)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b435d1",
   "metadata": {},
   "source": [
    "## Blueprint for training any PyTorch module\n",
    "Define a loss, initialize your optimizer, create the training loop and print the metrics (loss and accuracy) at the end of each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ac8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package has a variety of loss functions already implemented\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# nn package also has a variety of optimization algorithms implemented\n",
    "# we use the stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass over the model to get the logits \n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y == predicted).sum().float() / len(y)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # reset (zero) the gradients before running the backward pass over the model\n",
    "    # we need to do this because the gradients get accumulated at the same place across iterations\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params (weights and biases)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f702df6f",
   "metadata": {},
   "source": [
    "### Two layer neural network\n",
    "We now define a two layer (single hidden layer) neural network model using PyTorch and train it using stochastic gradient descent with the help of the autograd package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e5c70",
   "metadata": {},
   "source": [
    "Initialize the hyper-parameters like before. Play around with the learning rate and the regularization parameter to see their effect on the optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "lambda_l2 = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21141403",
   "metadata": {},
   "source": [
    "Create and print the two layer MLP with ReLU as the activation units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4133677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d93905",
   "metadata": {},
   "source": [
    "Again create the training loop and print the metrics (loss and accuracy) at the end of each iteration, the blueprint remains the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b517ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package has a variety of loss functions already implemented\n",
    "# we use cross entropy loss for our classification task\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# nn package also has a variety of optimization algorithms implemented\n",
    "# we use the stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# e = 1.  # plotting purpose\n",
    "\n",
    "# Training\n",
    "for t in range(1000):\n",
    "    \n",
    "    # Forward pass over the model to get the logits\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    loss = criterion(y_pred, y)\n",
    "    score, predicted = torch.max(y_pred, 1)\n",
    "    acc = (y == predicted).sum().float() / len(y)\n",
    "    print(\"[EPOCH]: %i, [LOSS]: %.6f, [ACCURACY]: %.3f\" % (t, loss.item(), acc))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # reset (zero) the gradients before running the backward pass over the model\n",
    "    # we need to do this because the gradients get accumulated at the same place across iterations\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params (weights and biases)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d854718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9ba35",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We will now demonstrate how to build a linear model and a neural network model for a regression task using PyTorch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "from plot_lib import plot_data, plot_model, set_default\n",
    "from matplotlib import pyplot as plt\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61450c6",
   "metadata": {},
   "source": [
    "### Create the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f189125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # num_samples_per_class\n",
    "D = 1  # dimensions\n",
    "C = 1  # num_classes\n",
    "H = 50  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1).to(device)\n",
    "y = X.pow(3) + 0.3 * torch.rand(X.size()).to(device)\n",
    "print(\"Shapes:\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.cpu().numpy(), y.cpu().numpy())\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c972f",
   "metadata": {},
   "source": [
    "### Linear model\n",
    "\n",
    "Initialize the values of the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "lambda_l2 = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481272f",
   "metadata": {},
   "source": [
    "Create the model and print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn package to create our linear model\n",
    "# each Linear module has a weight and bias\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "\n",
    "# print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04549577",
   "metadata": {},
   "source": [
    "Create the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use MSE (mean squared error) loss from the nn package for our regression task\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# we use the optim package to apply stochastic gradient descent for our parameter updates\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2\n",
    "\n",
    "# Training loop\n",
    "for t in range(1000):\n",
    "    \n",
    "    # forward pass over the model to get the logits (inputs to the loss function)\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss (MSE)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (t, loss.item()))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running the backward pass\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient of loss w.r.t our learnable params \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b881c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trained model\n",
    "plt.scatter(X.data.cpu().numpy(), y.data.cpu().numpy())\n",
    "plt.plot(X.data.cpu().numpy(), y_pred.data.cpu().numpy(), 'r-', lw=5)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8ea26",
   "metadata": {},
   "source": [
    "Play around with the values of the hyper-parameters and the value of H (number of hidden units) to observe the change in the models being learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c434c175",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "We will test the following assumptions pertaining to CNNs \n",
    "\n",
    "* Compositionality obtained using many layers\n",
    "* Locality + stationarity of images assumed by the convolutional layers\n",
    "* Invariance of object class to translations assumed by the pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from plot_lib import plot_data, plot_model, set_default\n",
    "\n",
    "set_default()\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eefd6ad",
   "metadata": {},
   "source": [
    "### Load the Dataset (MNIST)\n",
    "\n",
    "Load the MNIST handwritten digits dataset. We can use the PyTorch DataLoader utilities for this. This will download, shuffle, normalize data and arrange it in batches. Normalizing involves subtracting some coefficient (usually the mean) from each pixel values and dividing the resulting pixel values by another coefficient (usually the variance of the original pixel values). We also display some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4285d9",
   "metadata": {},
   "source": [
    "### Create the model classes\n",
    "For comparison purposes we will create two models classes: \n",
    "1. Multi-layer Perceptron\n",
    "2. Convolutional Neural Network\n",
    "\n",
    "Pay special attention to the order of the layer while creating CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)      \n",
    "        return self.network(x)\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_feature*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x) # Will it make a difference if we apply the non-linearity after the pooling layer?\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*4*4) # this is where are flattening the 2D feature maps into a single 1D vector so as to be used by the subsequent fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "def train(epoch, model, perm=torch.arange(0, 784).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send data to device, where the \"device\" is either a GPU if it exists or a CPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through the model\n",
    "        output = model(data)\n",
    "        # forward pass through the cross-entropy loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # backward pass through the cross-entropy loss function and the model\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # send data to device, where the \"device\" is either a GPU if it exists or a CPU\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # permute pixels\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477396c3",
   "metadata": {},
   "source": [
    "### Train a small MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8 # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24607665",
   "metadata": {},
   "source": [
    "### Train a ConvNet with the same number of parameters\n",
    "Play around with the hyper-parameters to understand their relationship with model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89318c69",
   "metadata": {},
   "source": [
    "### ConvNet performs better with the same number of parameters, thanks to its use of prior knowledge about images\n",
    "\n",
    "* Use of convolution: Locality and stationarity in images\n",
    "* Pooling: builds in some translation invariance\n",
    "\n",
    "### What happens if the assumptions are no longer true?\n",
    "Let us break the assumption of locality and permute the pixel within each image using an arbitrary permutation matrix. Also display the permuted images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(784)\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i in range(10):\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    # permute pixels\n",
    "    image_perm = image.view(-1, 28*28).clone()\n",
    "    image_perm = image_perm[:, perm]\n",
    "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(image_perm.squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c8282",
   "metadata": {},
   "source": [
    "### CNNs with permuted pixels\n",
    "What do you think will happen to CNNs when given permuted pixels as inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaaf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn, perm)\n",
    "    test(model_cnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7178a",
   "metadata": {},
   "source": [
    "### MLPs with permuted pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74033c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8    # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn, perm)\n",
    "    test(model_fnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710df90-5b33-4940-aa22-65a9dda17f6a",
   "metadata": {},
   "source": [
    "# Part 2: Recurrent Neural Networks (RNNs)\n",
    "We now build a recurrent neural network and train it on a synthetic and simple sequence level task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c154d5-3890-43a9-946e-44fde2f17bc5",
   "metadata": {},
   "source": [
    "### An example of many-to-one (sequence classification)\n",
    "\n",
    "This task is taken from Experiment 6(a) in the paper [Hochreiter & Schmidhuber (1997)](www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "The goal is to classify sequences.\n",
    "Elements and targets are represented locally (input vectors with only one non-zero bit).\n",
    "The sequence starts with an `B`, ends with a `E` (the “trigger symbol”), and otherwise consists of randomly chosen symbols from the set `{a, b, c, d}` except for two elements at positions `t1` and `t2` that are either `X` or `Y`.\n",
    "For the `DifficultyLevel.HARD` case, the sequence length is randomly chosen between `100` and `110`, `t1` is randomly chosen between `10` and `20`, and `t2` is randomly chosen between `50` and `60`.\n",
    "There are `4` sequence classes `Q`, `R`, `S`, and `U`, which depend on the temporal order of `X` and `Y`.\n",
    "\n",
    "The rules are:\n",
    "\n",
    "```\n",
    "X, X -> Q,\n",
    "X, Y -> R,\n",
    "Y, X -> S,\n",
    "Y, Y -> U.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ca405-37e6-42f6-8bae-1d2d153104cc",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be21c3-bc46-4157-a639-0b9e4fe5ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module that generates the sequence data according to the above described experiment\n",
    "from sequential_tasks import TemporalOrderExp6aSequence as QRSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f571f0-bbf4-4539-956c-cb7c426e0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "example_generator = QRSU.get_predefined_generator(\n",
    "    difficulty_level=QRSU.DifficultyLevel.EASY,\n",
    "    batch_size=16,\n",
    ")\n",
    "example_batch = example_generator[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d37385-08c8-4479-a60e-43b7a5acc2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated sequences and the labels in the current batch\n",
    "print(f'The return type is a {type(example_batch)} with length {len(example_batch)}.')\n",
    "print(f'The first item in the tuple is the batch of sequences with shape {example_batch[0].shape}.')\n",
    "print(f'The second item in the tuple is the corresponding batch of class labels with shape {example_batch[1].shape}.')\n",
    "print('The sequences generated and their corresponding labels in the batch are:')\n",
    "for i in range(example_batch[0].shape[0]):\n",
    "    input = example_batch[0][i]\n",
    "    output = example_batch[1][i]\n",
    "    sequence = example_generator.decode_x(input)\n",
    "    label = example_generator.decode_y(output)\n",
    "    print(sequence, ' ----- ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024ff26-b2f1-4eb8-a19a-cdad3e98d609",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebe0cc-7e7d-4edf-9aaa-df65352a9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set the random seed for reproducible results\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The RNN also returns its hidden state but we don't use it.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        h = self.rnn(x)[0]\n",
    "        x = self.linear(h)\n",
    "        return x\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.lstm(x)[0]\n",
    "        x = self.linear(h)\n",
    "        return x\n",
    "    \n",
    "    def get_states_across_time(self, x):\n",
    "        h_c = None\n",
    "        h_list, c_list = list(), list()\n",
    "        with torch.no_grad():\n",
    "            for t in range(x.size(1)):\n",
    "                h_c = self.lstm(x[:, [t], :], h_c)[1]\n",
    "                h_list.append(h_c[0])\n",
    "                c_list.append(h_c[1])\n",
    "            h = torch.cat(h_list)\n",
    "            c = torch.cat(c_list)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33071c98-fcaa-434a-9c0a-04293dc47a94",
   "metadata": {},
   "source": [
    "### Define the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb875fa-f7f5-4adc-9a3a-fd3765b83e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_gen, criterion, optimizer, device):\n",
    "    # Set the model to training mode. This will turn on layers that would\n",
    "    # otherwise behave differently during evaluation, such as dropout.\n",
    "    model.train()\n",
    "\n",
    "    # Store the number of sequences that were classified correctly\n",
    "    num_correct = 0\n",
    "\n",
    "    # Iterate over every batch of sequences. Note that the length of a data generator\n",
    "    # is defined as the number of batches required to produce a total of roughly 1000\n",
    "    # sequences given a batch size.\n",
    "    for batch_idx in range(len(train_data_gen)):\n",
    "\n",
    "        # Request a batch of sequences and class labels, convert them into tensors\n",
    "        # of the correct type, and then send them to the appropriate device.\n",
    "        data, target = train_data_gen[batch_idx]\n",
    "        data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "        # Perform the forward pass of the model\n",
    "        output = model(data)\n",
    "\n",
    "        # Pick only the output corresponding to last sequence element (input is pre padded)\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        # Compute the value of the loss for this batch. For loss functions like CrossEntropyLoss,\n",
    "        # the second argument is actually expected to be a tensor of class indices rather than\n",
    "        # one-hot encoded class labels. One approach is to take advantage of the one-hot encoding\n",
    "        # of the target and call argmax along its second dimension to create a tensor of shape\n",
    "        # (batch_size) containing the index of the class label that was hot for each sequence.\n",
    "        target = target.argmax(dim=1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Clear the gradient buffers of the optimized parameters.\n",
    "        # Otherwise, gradients from the previous batch would be accumulated.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the backward pass over the loss and the model\n",
    "        loss.backward()\n",
    "\n",
    "        # the gradient step to update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred = output.argmax(dim=1)\n",
    "        num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c794de0-6db6-422f-b16b-1460893e2eda",
   "metadata": {},
   "source": [
    "### Define the Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4c0b33-3781-46fd-984c-b74194c6f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_gen, criterion, device):\n",
    "    # Set the model to evaluation mode. This will turn off layers that would\n",
    "    # otherwise behave differently during training, such as dropout.\n",
    "    model.eval()\n",
    "\n",
    "    # Store the number of sequences that were classified correctly\n",
    "    num_correct = 0\n",
    "\n",
    "    # A context manager is used to disable gradient calculations during inference\n",
    "    # to reduce memory usage, as we typically don't need the gradients at this point.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(test_data_gen)):\n",
    "            data, target = test_data_gen[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            # Pick only the output corresponding to last sequence element (input is pre padded)\n",
    "            output = output[:, -1, :]\n",
    "\n",
    "            target = target.argmax(dim=1)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce87ae-33c8-45b7-bdde-750a7d83754c",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870092a-05e6-458b-abbe-cfff6b4da588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from plot_lib import set_default, plot_state, print_colourbar\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660e5f5-4f06-42a9-be6b-55d088ca2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True):\n",
    "    # Automatically determine the device that PyTorch should use for computation\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train = {'loss': [], 'acc': []}\n",
    "    history_test = {'loss': [], 'acc': []}\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Run the training loop and calculate the accuracy.\n",
    "        # Remember that the length of a data generator is the number of batches,\n",
    "        # so we multiply it by the batch size to recover the total number of sequences.\n",
    "        num_correct, loss = train(model, train_data_gen, criterion, optimizer, device)\n",
    "        accuracy = float(num_correct) / (len(train_data_gen) * train_data_gen.batch_size) * 100\n",
    "        history_train['loss'].append(loss)\n",
    "        history_train['acc'].append(accuracy)\n",
    "\n",
    "        # Do the same for the testing loop\n",
    "        num_correct, loss = test(model, test_data_gen, criterion, device)\n",
    "        accuracy = float(num_correct) / (len(test_data_gen) * test_data_gen.batch_size) * 100\n",
    "        history_test['loss'].append(loss)\n",
    "        history_test['acc'].append(accuracy)\n",
    "\n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[Epoch {epoch + 1}/{max_epochs}]'\n",
    "                  f\" loss: {history_train['loss'][-1]:.4f}, acc: {history_train['acc'][-1]:2.2f}%\"\n",
    "                  f\" - test_loss: {history_test['loss'][-1]:.4f}, test_acc: {history_test['acc'][-1]:2.2f}%\")\n",
    "\n",
    "    # Generate diagnostic plots for the loss and accuracy\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(9, 4.5))\n",
    "    for ax, metric in zip(axes, ['loss', 'acc']):\n",
    "        ax.plot(history_train[metric])\n",
    "        ax.plot(history_test[metric])\n",
    "        ax.set_xlabel('epoch', fontsize=12)\n",
    "        ax.set_ylabel(metric, fontsize=12)\n",
    "        ax.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832eba41-2dee-4dbe-8447-d84bd5043962",
   "metadata": {},
   "source": [
    "### Train Elman RNNs for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d4e06-2f9c-4fde-b847-91ecf0e5ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.EASY\n",
    "batch_size     = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 10\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4bc51e-0ab7-4540-8ceb-9043cc6885ac",
   "metadata": {},
   "source": [
    "### Train LSTMs for 10 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43f1c70-a286-492d-8f38-343e9ab3a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.EASY\n",
    "batch_size     = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 10\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aef30a-eb41-4549-81d2-f10677121aaf",
   "metadata": {},
   "source": [
    "### Elman: Increasing epoch to 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfea853-9182-41e9-96a1-dc71b7414a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.EASY\n",
    "batch_size     = 4\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 32\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 50\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b79b1-c52b-43cb-8bad-1368790805e5",
   "metadata": {},
   "source": [
    "### LSTMs: Increasing epoch to 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac36ac5-2b9b-4ee2-948f-1034014c0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.EASY\n",
    "batch_size     = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 30\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eefeee-8534-49a9-999f-d7b5f020fd29",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48567711-49cc-4481-a562-381ba5300aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "def evaluate_model(model, difficulty, seed=9001, verbose=False):\n",
    "    # Define a dictionary that maps class indices to labels\n",
    "    class_idx_to_label = {0: 'Q', 1: 'R', 2: 'S', 3: 'U'}\n",
    "\n",
    "    # Create a new data generator\n",
    "    data_generator = QRSU.get_predefined_generator(difficulty, seed=seed)\n",
    "\n",
    "    # Track the number of times a class appears\n",
    "    count_classes = collections.Counter()\n",
    "\n",
    "    # Keep correctly classified and misclassified sequences, and their\n",
    "    # true and predicted class labels, for diagnostic information.\n",
    "    correct = []\n",
    "    incorrect = []\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(data_generator)):\n",
    "            data, target = test_data_gen[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "            data_decoded = data_generator.decode_x_batch(data.numpy())\n",
    "            target_decoded = data_generator.decode_y_batch(target.numpy())\n",
    "\n",
    "            output = model(data)\n",
    "            sequence_end = torch.tensor([len(sequence) for sequence in data_decoded]) - 1\n",
    "            output = output[torch.arange(data.shape[0]).long(), sequence_end, :]\n",
    "\n",
    "            target = target.argmax(dim=1)\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            y_pred_decoded = [class_idx_to_label[y.item()] for y in y_pred]\n",
    "\n",
    "            count_classes.update(target_decoded)\n",
    "            for i, (truth, prediction) in enumerate(zip(target_decoded, y_pred_decoded)):\n",
    "                if truth == prediction:\n",
    "                    correct.append((data_decoded[i], truth, prediction))\n",
    "                else:\n",
    "                    incorrect.append((data_decoded[i], truth, prediction))\n",
    "\n",
    "    num_sequences = sum(count_classes.values())\n",
    "    accuracy = float(len(correct)) / num_sequences * 100\n",
    "    print(f'The accuracy of the model is measured to be {accuracy:.2f}%.\\n')\n",
    "\n",
    "    # Report the accuracy by class\n",
    "    for label in sorted(count_classes):\n",
    "        num_correct = sum(1 for _, truth, _ in correct if truth == label)\n",
    "        print(f'{label}: {num_correct} / {count_classes[label]} correct')\n",
    "\n",
    "    # Report some random sequences for examination\n",
    "    print('\\nHere are some example sequences:')\n",
    "    for i in range(10):\n",
    "        sequence, truth, prediction = correct[random.randrange(0, 10)]\n",
    "        print(f'{sequence} -> {truth} was labelled {prediction}')\n",
    "\n",
    "    # Report misclassified sequences for investigation\n",
    "    if incorrect and verbose:\n",
    "        print('\\nThe following sequences were misclassified:')\n",
    "        for sequence, truth, prediction in incorrect:\n",
    "            print(f'{sequence} -> {truth} was labelled {prediction}')\n",
    "    else:\n",
    "        print('\\nThere were no misclassified sequences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7c3de-b799-48aa-bc29-6fbe3d7062ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, difficulty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
