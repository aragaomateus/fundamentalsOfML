{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 4: Logistic Regression Recap, Multi-class Classification and Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "# Install autograd:\n",
    "#!conda install -c conda-forge autograd\n",
    "\n",
    "import autograd.numpy as numpy\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from autograd import grad,elementwise_grad\n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "# For the plots in this notebook we will use a widget to get interactive and better looking plots! \n",
    "# follow these instructions to set up matplotlib widget for Jupyter Lab\n",
    "# https://github.com/matplotlib/jupyter-matplotlib#installation\n",
    "# Specifically you will need to install the widget using : conda install -c conda-forge ipympl\n",
    "# To enable the widget run the following two commands :\n",
    "# jupyter nbextension install --py --symlink --sys-prefix --overwrite ipympl\n",
    "#jupyter nbextension enable --py --sys-prefix ipympl\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General helper method to convert sci-kit datasets to Pandas DataFrame.\n",
    "def sklearn_to_df(sklearn_dataset):\n",
    "    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n",
    "    df['target'] = pd.Series(sklearn_dataset.target)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Logistic Regression & Different Metrics\n",
    "\n",
    "We will now implement and train another logistic regression model using Sci-kit learn. The goal will be to implement the model, which given a new data point infers the probability of breast cancer. \n",
    "\n",
    "Helper method below copied from: [Helper Method](https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General helper method to convert sci-kit datasets to Pandas DataFrame.\n",
    "def sklearn_to_df(sklearn_dataset):\n",
    "    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n",
    "    df['target'] = pd.Series(sklearn_dataset.target)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's eyeball the data a little bit in a quick and hacky manner. Always a good practice to see what the data looks like. **The training data of course!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset = datasets.load_breast_cancer() # Load the data and convert to a pandas dataframe\n",
    "df = sklearn_to_df(cancer_dataset)\n",
    "\n",
    "print(df.head()) # Print out the first five data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather a few summary statistics about our data. Again, always a good practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(df) # The number of data points.\n",
    "print('N = {:d} data points'.format(N))\n",
    "\n",
    "# Give a barplot of each class.\n",
    "plt.figure()\n",
    "plt.bar([0,1], df['target'].value_counts(ascending = True), color = ['r', 'b'], tick_label = cancer_dataset.target_names)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Cancer Dataset: Class Counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is unbalanced because there are more examples of benign cancer than malignant.  This is typical of many real-life datasets where we are sometimes limited in how many training examples we have.  Let's split our data into a training and validation set.  We'll use a 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data.  DO NOT TOUCH THE TEST DATA FROM HERE ON!!\n",
    "train_data, val_data = model_selection.train_test_split(df, test_size = 0.2) # 0.2 is 20% validation data.\n",
    "\n",
    "# Split the features from the class labels.\n",
    "X_train = train_data.drop('target', axis = 1) # We drop the target from the features.  \n",
    "X_val  = val_data.drop('target', axis = 1)  # Note that this does not operate inplace.\n",
    " \n",
    "y_train = train_data['target']\n",
    "y_val  = val_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is loaded and split we can train a logistic regression model.  For the optimization we use the \"liblinear\" solver.  There are many other solvers that are also available, such as Newton CG for example.  For more information there is a nice stackexchange post here: [Solvers](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit a logistic regression model.\n",
    "model = LogisticRegression(solver = 'liblinear', class_weight = 'balanced')\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained so we can validate it on our validation set.  The Sci-kit metrics module contains many useful functions for this purpose.  We try out a few of them below. \n",
    "\n",
    "Let us first briefly explain some of these metrics we will use.  \n",
    "\n",
    "#### Accuracy \n",
    "Accuracy is obviously the percentage of all correctly classified examples in our test set.  \n",
    "\n",
    "#### Confusion Matrix\n",
    "The confusion matrix is the following matrix:\n",
    "$$\n",
    "C = \\begin{bmatrix}\n",
    "\\text{Predict 0, Actual 0} & \\text{Predict 0, Actual 1}\\\\\n",
    "\\text{Predict 1, Actual 0} & \\text{Predict 1, Actual 1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Notice that the diagonal entries are the examples that are correctly classified.  \n",
    "\n",
    "#### Precision Score\n",
    "The precision score is the percentage \n",
    "$$\n",
    "\\text{Precision } = \\frac{C_{00}}{C_{00} + C_{01}}.\n",
    "$$\n",
    "So it is the percentage of predicted malignant tumors that we classify correctly.  \n",
    "\n",
    "#### Recall Score \n",
    "The recall score is the percentage\n",
    "$$\n",
    "\\text{Recall } = \\frac{C_{00}}{C_{00} + C_{10}}\n",
    "$$\n",
    "So it is the percentage of malignant tumors that we classify correctly. Note that Precision and Recall are two different quantities.\n",
    "\n",
    "**Using multiple evaluation metrics helps give a better picture of how well our classifier is doing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_val)\n",
    "\n",
    "# See the percentage of examples that are correctly classified.\n",
    "accuracy = metrics.accuracy_score(y_val, pred) \n",
    "print(\"Accuracy = {:0.1f}%\".format(accuracy * 100))\n",
    "\n",
    "# The matrix of predictions and true values for each class.\n",
    "conf_matrix = metrics.confusion_matrix(y_val, pred)\n",
    "print(\"Confusion matrix = \")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Precision score.\n",
    "precision = metrics.precision_score(y_val, pred)\n",
    "print(\"Precision = {:0.1f}%\".format(100 * precision))\n",
    "\n",
    "# Recall score.\n",
    "recall = metrics.recall_score(y_val, pred)\n",
    "print(\"Recall    = {:0.1f}%\".format(100 * recall))\n",
    "\n",
    "# MCC Score.\n",
    "mcc = metrics.matthews_corrcoef(y_val, pred)\n",
    "print(\"MCC = {:0.2f}\".format(mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also plot the ROC curve and use it to estimate thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = model.predict_proba(X_val)\n",
    "metrics.RocCurveDisplay.from_predictions(y_val, pred_probs[:,1])\n",
    "\n",
    "# or alternatively\n",
    "metrics.RocCurveDisplay.from_estimator(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we have highly imbalanced classes, we can plot the PR curve as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.PrecisionRecallDisplay.from_predictions(y_val, pred_probs[:,1])\n",
    "\n",
    "# or alternatively\n",
    "metrics.PrecisionRecallDisplay.from_estimator(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2 : Multiclass Classification\n",
    "\n",
    "Multiclass classification is just when we have more than 2 classes.  Instead of modeling the posterior class probabilities as Bernoulli random variables, we can model them as multinomial random variables for example.  The example we'll consider here is the classic Iris dataset by Fisher (a prominent early statistician).  Let's load the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and convert it into a Pandas dataframe.\n",
    "iris_data = datasets.load_iris()\n",
    "df = sklearn_to_df(iris_data)\n",
    "\n",
    "# Print the first 5 examples to see how the data looks.\n",
    "print(\"N = \", len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test\n",
    "\n",
    "We'll use a 80/20 split for our training/test sets. We will not touch the test set. For validation purposes we will use cross validation. This is particularly useful in this situation since we only have 150 examples in our dataset. We will use $k$-fold cross-validation with $k=10$.  This works by splitting the training data into 10 disjoint sets.  We then leave out one of these sets and train our model on the remaining 9 sets and treat the left out set as a validation set.  We repeat this a total of 10 times and average the validation scores.  If $k = N$ is the number of samples, then this is also called leave one out cross-validation (LOOCV).  Note that using $k$-fold cross-validation is more expensive than if we had a separate validation set to begin with because we have to train our model many times.\n",
    "\n",
    "Cross-validation will become more useful when we have a hyperparameter to control during training and want to see which value of the hyperparameter will give the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data.  DO NOT TOUCH THE TEST DATA FROM HERE ON!!\n",
    "train_data, test_data = model_selection.train_test_split(df, test_size = 0.2) # 0.2 is 20% test data.\n",
    "\n",
    "# Split the features from the class labels.\n",
    "X_train = train_data.drop('target', axis = 1) # We drop the target from the features.  \n",
    "X_test  = test_data.drop('target', axis = 1)  # Note that this does not operate inplace.\n",
    " \n",
    "y_train = train_data['target']\n",
    "y_test  = test_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compare 2 logistic regression models using cross-validation.  The first model is trained on all of the features whereas the second model is only trained on \"sepal length\" and \"sepal width\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 2 different models.\n",
    "model1 = LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "model2 = LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg')\n",
    "\n",
    "X_train2 = X_train[['sepal length (cm)', 'sepal width (cm)']]\n",
    "\n",
    "# k-fold cross-validation.\n",
    "k = 10\n",
    "splitter = model_selection.KFold(k)\n",
    "\n",
    "# Compute cross-validation scores.\n",
    "cv_scores1 = model_selection.cross_val_score(model1, X_train, y_train, cv = splitter)\n",
    "cv_scores2 = model_selection.cross_val_score(model2, X_train2, y_train, cv = splitter)\n",
    "\n",
    "# Print the means of the scores.\n",
    "print(\"Model 1 CV score = \", np.mean(cv_scores1))\n",
    "print(\"Model 2 CV score = \", np.mean(cv_scores2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "\n",
    "We can also look to see how our two models perform when given additional training examples.  A learning curve shows the tradeoff between the number of samples we use to train and the model's predictive accuracy.  We plot the learning curves for our two models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,4))\n",
    "\n",
    "train_sizes1, train_scores1, test_scores1 = model_selection.learning_curve(model1, X_train, y_train, cv = splitter)\n",
    "train_sizes2, train_scores2, test_scores2 = model_selection.learning_curve(model2, X_train2, y_train, cv = splitter)\n",
    "\n",
    "train_scores_mean1 = np.mean(train_scores1, axis = 1)\n",
    "test_scores_mean1  = np.mean(test_scores1, axis = 1)\n",
    "train_scores_std1  = np.std(train_scores1, axis = 1)\n",
    "test_scores_std1   = np.std(test_scores1, axis = 1)\n",
    "\n",
    "train_scores_mean2 = np.mean(train_scores2, axis = 1)\n",
    "test_scores_mean2  = np.mean(test_scores2, axis = 1)\n",
    "train_scores_std2  = np.std(train_scores2, axis = 1)\n",
    "test_scores_std2   = np.std(test_scores2, axis = 1)\n",
    "\n",
    "ax1.set_title('Logistic Regression Model 1')\n",
    "ax1.plot(train_sizes1, train_scores_mean1, 'o-', label = 'Training')\n",
    "ax1.plot(train_sizes1, test_scores_mean1, 's-', label = 'Validation')\n",
    "ax1.fill_between(train_sizes1, train_scores_mean1 - train_scores_std1, train_scores_mean1 + train_scores_std1, alpha = 0.1)\n",
    "ax1.fill_between(train_sizes1, test_scores_mean1 - test_scores_std1, test_scores_mean1 + test_scores_std1, alpha = 0.1)\n",
    "ax1.set_xlabel('Training Examples')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title('Logistic Regression Model 2')\n",
    "ax2.plot(train_sizes2, train_scores_mean2, 'o-', label = 'Training')\n",
    "ax2.plot(train_sizes2, test_scores_mean2, 's-', label = 'Validation')\n",
    "ax2.fill_between(train_sizes2, train_scores_mean2 - train_scores_std2, train_scores_mean2 + train_scores_std2, alpha = 0.1)\n",
    "ax2.fill_between(train_sizes2, test_scores_mean2 - test_scores_std2, test_scores_mean2 + test_scores_std2, alpha = 0.1)\n",
    "ax2.set_xlabel('Training Examples')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the models.\n",
    "\n",
    "Finally, we actually evaluate our two models on our test set.  The first model has a higher cross-validation score so we expect it to do better on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train2, y_train)\n",
    "\n",
    "pred1 = model1.predict(X_test)\n",
    "\n",
    "X_test2 = X_test[['sepal length (cm)', 'sepal width (cm)']]\n",
    "pred2 = model2.predict(X_test2)\n",
    "\n",
    "# Get the percent of correctly classified results.\n",
    "acc1 = metrics.accuracy_score(pred1, y_test)\n",
    "acc2 = metrics.accuracy_score(pred2, y_test)\n",
    "\n",
    "print(\"Logistic regression model accuracy (all features) = {:0.1f}%\".format(100 * acc1))\n",
    "print(\"Logistic regression model accuracy (2 features)   = {:0.1f}%\".format(100 * acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'd expect, training with more features gives us a more flexible model.  Excluding the remaining features is equivalent to setting their weights to 0.  Thus, we are solving a constrained minimization problem when we train, which will worse than solving the unconstrained problem where we are using all of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 : Support Vector Machines\n",
    "We will now play around with the support vector machine. We will first compare them to a standard logistic regression model. Then we will see how they work on datasets which are not linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the packages we'll need.\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Linear SVMs\n",
    "We will implement a linear SVM for the task of breast cancer classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the dataset and split it into training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set a random seed first.\n",
    "np.random.seed(19)\n",
    "\n",
    "# First load the data.\n",
    "data = datasets.load_breast_cancer()\n",
    "X = data.data   # The features.\n",
    "y = data.target # The targets.\n",
    "\n",
    "# Print the dataset sizes.\n",
    "print('Shape of X = ', X.shape)\n",
    "print('Shape of y = ', y.shape)\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size = 0.33, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a linear SVM using scikit-learn. In addition also train a logistic regression model also using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the linear SVM.\n",
    "\n",
    "svm = LinearSVC(dual = False) # Uses the squared-hinge loss function when fitting the model.\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# NOTE : We can also try using the hinge loss, by specifying loss='hinge' and setting dual to True. \n",
    "# The squared_hinge loss is more commonly used to penalize outliers more severely, does not use random states and has a smoother loss curve.\n",
    "\n",
    "# Now evaluate it on the test points.\n",
    "y_pred = svm.predict(X_val)\n",
    "\n",
    "acc = metrics.accuracy_score(y_val, y_pred)\n",
    "print('Linear SVM validation accuracy = {:0.1f}%'.format(100*acc))\n",
    "\n",
    "# Compare to a simple logistic regression model.\n",
    "\n",
    "lr = LogisticRegression(solver = 'liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "acc = metrics.accuracy_score(y_val, y_pred)\n",
    "print('Logistic Regression validation accuracy = {:0.1f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a basic linear SVM already does very well and is comparable to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare logistic regression with SVM for binary classification.  Suppose that our data has $n$ features and we have trained a logistic regression model with weight vector $\\theta_{\\mathrm{LR}}\\in \\mathbb{R}^{n+1}$ as well as a linear SVM with parameters $w_{\\mathrm{SVM}}\\in \\mathbb{R}^n$, and $b_{\\mathrm{SVM}}\\in \\mathbb{R}$.  Assume that the optimal parameters have been found in both cases.  If the data is linearly separable then is it true that $\\theta_{\\mathrm{LR}} = (b_{\\mathrm{SVM}}, w_{\\mathrm{SVM}})$? \n",
    "\n",
    "If not then why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**They will not be the same in general.**\n",
    "\n",
    "Consider the dataset $X = (-5, -1, 1, 2)^T$ and $y = (-1, -1, 1, 1)^T$ (equivalently $y = (0,0,1,1)^T$ for logistic regression).  The SVM parameters will just be $(b_{\\mathrm{SVM}}, w_{\\mathrm{SVM}}) = (0,1)$.  However, the logistic regression parameters will be slightly different.  Recall the gradient of the loss function for logistic regression is\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta) = -\\sum_{i=1}^4 (y_i - h(X_i;\\theta))X_i\n",
    "$$\n",
    "where\n",
    "$$\n",
    "h(x;\\theta) = \\frac{1}{1 + \\exp(-\\theta^Tx)}\n",
    "$$\n",
    "However, if we plug all of the values into the gradient we get\n",
    "$$\n",
    "\\nabla_{\\theta}J((b_{\\mathrm{SVM}}, w_{\\mathrm{SVM}})) = -\\left( \\frac{5}{1+e^{5}} + \\frac{1}{1+e^{1}} + \\frac{e^{-1}}{1+e^{-1}} +  \\frac{2e^{-2}}{1 + e^{-2}} \\right) \\neq 0\n",
    "$$\n",
    "Since the gradient is non-zero we know that $(b_{\\mathrm{SVM}}, w_{\\mathrm{SVM}})$ is not the optimal value for the logistic regression loss function and hence cannot be $\\theta_{\\mathrm{LR}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case of non-linearly separable dataset\n",
    "\n",
    "If the data is linear separable, then a linear SVM should be able to achieve 100% accuracy.  However, this is rarely the case since even the breast cancer dataset was not exactly linear separable.  We'll use a synthetic dataset to illustrate this.  This data is drawn from a bi-modal Gaussian mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    N : the number of data points\n",
    "\n",
    "Output:\n",
    "    X, y : the features and targets of shapes (N,2) and (N, )\n",
    "\"\"\"\n",
    "def sample_bimodal_data(N):\n",
    "    \n",
    "    # The two modes and covariances.\n",
    "    mu1 = np.asarray([-1, 0])\n",
    "    mu2 = np.asarray([1, 0])\n",
    "    \n",
    "    cov1 = 2.5 * np.identity(2)\n",
    "    cov2 = 1.5 * np.identity(2)\n",
    "    \n",
    "    N1 = N//2   # Number of points in first class.\n",
    "    N2 = N - N1 # Number of points in second class.\n",
    "    \n",
    "    # Sample the random points.\n",
    "    X1 = np.random.multivariate_normal(mu1, cov1, N1)\n",
    "    X2 = np.random.multivariate_normal(mu2, cov2, N2)\n",
    "    Y1 = np.zeros(N1)\n",
    "    Y2 = np.ones(N2)\n",
    "    \n",
    "    # Combine the data.\n",
    "    X = np.vstack((X1, X2))\n",
    "    Y = np.concatenate((Y1, Y2), axis = None)\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sample data.\n",
    "N = 100\n",
    "X,Y = sample_bimodal_data(N)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(X[:N//2, 0], X[:N//2, 1], label = 'Class 0')\n",
    "plt.scatter(X[N - N//2:, 0], X[N - N//2:, 1], label = 'Class 1')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Sample Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the factor in front of the covariances or shifting the centers of the two distributions to be closer to each other will cause the data to overlap more, making it harder to classify. Lets try that! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a slack variable C\n",
    "\n",
    "Since the data is not perfectly linearly separable you'll want to use a slack variable which allows SVM to handle this dataset.  Let's train some models with different values of $C$ and compare them using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the data and split it into training and testing.\n",
    "\n",
    "N = 1000\n",
    "X, Y = sample_bimodal_data(N)\n",
    "\n",
    "# Use a 70/30 split\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(X, Y, test_size = 0.30, random_state = 981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVM model to use with a slack variable\n",
    "svm = LinearSVC(C = 1e-1, dual = False)\n",
    "svm.fit(X_train, Y_train)\n",
    "svmpred = svm.predict(X_val)\n",
    "acc = metrics.accuracy_score(Y_val, svmpred)\n",
    "print('SVM accuracy = {:0.1f}%'.format(100*acc))\n",
    "\n",
    "plt.figure(2)\n",
    "I = svmpred == 0\n",
    "plt.scatter(X_val[I, 0], X_val[I, 1], label = 'Predicted class 0')\n",
    "I = svmpred == 1\n",
    "plt.scatter(X_val[I, 0], X_val[I, 1], label = 'Predicted class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with various different mu1 values and demonstrate SVM accuracy gets worse as mu1 and mu2 get closer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train some models with different $C$ and compare them use cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different SVM models to use\n",
    "svm_1 = LinearSVC(C = 10, dual = False)\n",
    "svm_2 = LinearSVC(C = 1, dual = False)\n",
    "svm_3 = LinearSVC(C = 1e-3, dual = False)\n",
    "svm_4 = LinearSVC(C = 1e-7, dual = False)\n",
    "\n",
    "split = model_selection.KFold(5)\n",
    "# Get the CV scores.\n",
    "cv_1 = model_selection.cross_val_score(svm_1, X_train, Y_train, cv = split)\n",
    "cv_2 = model_selection.cross_val_score(svm_2, X_train, Y_train, cv = split)\n",
    "cv_3 = model_selection.cross_val_score(svm_3, X_train, Y_train, cv = split)\n",
    "cv_4 = model_selection.cross_val_score(svm_4, X_train, Y_train, cv = split)\n",
    "\n",
    "# Print the average scores.\n",
    "print('C = 10    CV average score = {:0.1f}%'.format(np.mean(cv_1) * 100))\n",
    "print('C = 1     CV average score = {:0.1f}%'.format(np.mean(cv_2) * 100))\n",
    "print('C = 1e-3  CV average score = {:0.1f}%'.format(np.mean(cv_3) * 100))\n",
    "print('C = 1e-7  CV average score = {:0.1f}%'.format(np.mean(cv_4) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model performs slightly differently for different values of the slack variable $C$.  \n",
    "\n",
    "$$\n",
    "\\min_{w,b,\\zeta} \\frac{1}{2}w^Tw + C\\sum_{i=1}^n \\zeta_i,\\quad \\text{ such that }\\quad y_i(w^Tx_i + b) \\ge 1 - \\zeta_i,\\quad \\zeta_i \\ge 0\n",
    "$$\n",
    "\n",
    "See the sci-kit [documentation](https://scikit-learn.org/stable/modules/svm.html) for more details.  We can also plot a curve of the validation score for many different $C$ values which can be helpful for determining the optimal hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the C values we want to look at.\n",
    "C = 1/(2**np.arange(0, 20)) # 1,...,1e-6\n",
    "\n",
    "k = 10 # Kfold CV.\n",
    "cv_scores = np.zeros(len(C))\n",
    "split = model_selection.KFold(k)\n",
    "for i in range(len(C)):\n",
    "    svm = LinearSVC(C = C[i], dual = False)\n",
    "    cv_scores[i] = np.mean(model_selection.cross_val_score(svm, X_train, Y_train, cv = split))\n",
    "\n",
    "plt.figure(2)\n",
    "plt.semilogx(C, cv_scores, 'b-x')\n",
    "plt.xlabel(r'$C$')\n",
    "plt.ylabel(r'Score')\n",
    "plt.title(r'{:d}-Fold CV Score for Linear SVM'.format(k))\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another non-linearly separable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    N : the number of data points\n",
    "\n",
    "Output:\n",
    "    X, y : the features and targets of shapes (N,2) and (N, )\n",
    "\"\"\"\n",
    "def gen_data1(N):\n",
    "    N1 = N//2\n",
    "    N2 = N - N1\n",
    "    t = np.linspace(0, 2*np.pi, N1)\n",
    "    \n",
    "    X1 = np.zeros((N1, 2))\n",
    "    X1[:,0] = 4*np.cos(t) + 0.1*np.random.randn(N1)\n",
    "    X1[:,1] = 4*np.sin(t) + 0.1*np.random.randn(N1)\n",
    "    y1 = np.zeros(N1)\n",
    "    \n",
    "    X2 = np.random.randn(2*N2)\n",
    "    X2 = X2.reshape((N2, 2))\n",
    "    y2 = np.ones(N2)\n",
    "\n",
    "    # Combine the data.\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.concatenate((y1, y2), axis = None) # axis = None means that arrays flattened before use\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data.\n",
    "N = 1000\n",
    "X, Y = gen_data1(N)\n",
    "\n",
    "plt.figure(3)\n",
    "plt.scatter(X[:N//2, 0], X[:N//2, 1], label = 'Class 0')\n",
    "plt.scatter(X[N - N//2:, 0], X[N - N//2:, 1], label = 'Class 1')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Sample Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different SVM models to use\n",
    "# Use a 70/30 split\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(X, Y, test_size = 0.3, random_state = 981)\n",
    "svm = LinearSVC(C = 1e+10, dual = False)\n",
    "svm.fit(X_train, Y_train)\n",
    "svmpred = svm.predict(X_val)\n",
    "acc = metrics.accuracy_score(Y_val, svmpred)\n",
    "print('SVM accuracy = {:0.1f}%'.format(100*acc))\n",
    "\n",
    "plt.figure(2)\n",
    "I = svmpred == 0\n",
    "plt.scatter(X_val[I, 0], X_val[I, 1], label = 'predicted class 0')\n",
    "I = svmpred == 1\n",
    "plt.scatter(X_val[I, 0], X_val[I, 1], label = 'prediced class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data is not linearly separable although we can very clearly see some separation. If we transform the data by only looking at the radius, then we would be able to linearly separate the data. We will visit this in the next lecture when we talk about kernel SVM's which are much more flexible models and can handle a wider array of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return (rho, phi)\n",
    "\n",
    "pX = np.vstack(cart2pol(X[:, 0], X[:, 1])).T\n",
    "print(pX.shape)\n",
    "plt.figure(4)\n",
    "plt.scatter(pX[:N//2, 0], pX[:N//2, 1], label = 'Class 0')\n",
    "plt.scatter(pX[N - N//2:, 0], pX[N - N//2:, 1], label = 'Class 1')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$p_1$ (radius)')\n",
    "plt.ylabel(r'$p_2$ (angle)')\n",
    "plt.title('Sample Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different SVM models to use\n",
    "# Use a 70/30 split\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(pX, Y, test_size = 0.3, random_state = 981)\n",
    "svm = LinearSVC(C = 1e+10, dual = False)\n",
    "svm.fit(X_train, Y_train)\n",
    "svmpred = svm.predict(X_val)\n",
    "acc = metrics.accuracy_score(Y_val, svmpred)\n",
    "print('SVM accuracy = {:0.1f}%'.format(100*acc))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
