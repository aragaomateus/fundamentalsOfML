{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 \n",
    "\n",
    "Columns represent (in order):\n",
    "1) Diabetes status (1 = has been diagnosed with diabetes, 0 = has not)\n",
    "2) High blood pressure (1 = has been diagnosed with hypertension, 0 = has not)\n",
    "3) High cholesterol (1 = has been diagnosed with high cholesterol, 0 = has not)\n",
    "4) Body Mass Index (weight / height2)\n",
    "5) Smoker (1 = person has smoked more than 100 cigarettes in their life, 0 = has not)\n",
    "6) Stroke (1 = person has previously suffered a stroke, 0 = has not)\n",
    "7) Myocardial issues (1 = has previously had a heart attack, 0 = has not)\n",
    "8) Physically active (1 = person describes themselves as physically active, 0 = does not)\n",
    "9) Eats fruit (1 = person reports eating fruit at least once a day, 0 = does not)\n",
    "10) Eats vegetables (1 = person reports eating vegetables at least once a day, 0 = does not)\n",
    "11) Heavy Drinker (1 = consumes more drinks than the CDC threshold/week, 0 = does not)\n",
    "12) Has healthcare (1 = person has some kind of healthcare plan coverage, 0 = does not)\n",
    "13) NotAbleToAffordDoctor (1 = person needed to see the doctor within the last year, but\n",
    "could not afford to, 0 = did not)\n",
    "14) General health: Self-assessment of health status on a scale from 1 to 5\n",
    "15) Mental health: Days of poor mental health in the last 30 days (self-assessed)\n",
    "16) Physical health: Days of poor physical health in the last 30 days (self-assessed)\n",
    "17) Hard to climb stairs (1 = person reports difficulties in climbing stairs, 0 = does not)\n",
    "18) Biological sex (1 = male, 2 = female)\n",
    "19) Age bracket (1 = 18-24, 2 = 25-29, 3 = 30-34, 4 = 35-39, 5 = 40-44, 6 = 45-49, 7 = 50-54,\n",
    "8 = 55-59, 9 = 60-64, 10 = 65-69, 11 = 70-74, 12 = 75-79, 13 = 80+)\n",
    "20) Education bracket (terminal education is 1 = only kindergarten, 2 = elementary school,\n",
    "3 = some high school, 4 = GED, 5 = some college, 6 = college graduate)\n",
    "21) Income bracket (Annual income where 1 = below $10k, 8 = above $75k)\n",
    "22) Zodiac sign (Tropical calendar, 1 = Aries, 12 = Pisces, with everything else in between)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>Myocardial</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruit</th>\n",
       "      <th>Vegetables</th>\n",
       "      <th>...</th>\n",
       "      <th>NotAbleToAffordDoctor</th>\n",
       "      <th>GeneralHealth</th>\n",
       "      <th>MentalHealth</th>\n",
       "      <th>PhysicalHealth</th>\n",
       "      <th>HardToClimbStairs</th>\n",
       "      <th>BiologicalSex</th>\n",
       "      <th>AgeBracket</th>\n",
       "      <th>EducationBracket</th>\n",
       "      <th>IncomeBracket</th>\n",
       "      <th>Zodiac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diabetes  HighBP  HighChol  BMI  Smoker  Stroke  Myocardial  PhysActivity  \\\n",
       "0         0       1         1   40       1       0           0             0   \n",
       "1         0       0         0   25       1       0           0             1   \n",
       "2         0       1         1   28       0       0           0             0   \n",
       "3         0       1         0   27       0       0           0             1   \n",
       "4         0       1         1   24       0       0           0             1   \n",
       "\n",
       "   Fruit  Vegetables  ...  NotAbleToAffordDoctor  GeneralHealth  MentalHealth  \\\n",
       "0      0           1  ...                      0              5            18   \n",
       "1      0           0  ...                      1              3             0   \n",
       "2      1           0  ...                      1              5            30   \n",
       "3      1           1  ...                      0              2             0   \n",
       "4      1           1  ...                      0              2             3   \n",
       "\n",
       "   PhysicalHealth  HardToClimbStairs  BiologicalSex  AgeBracket  \\\n",
       "0              15                  1              1           9   \n",
       "1               0                  0              1           7   \n",
       "2              30                  1              1           9   \n",
       "3               0                  0              1          11   \n",
       "4               0                  0              1          11   \n",
       "\n",
       "   EducationBracket  IncomeBracket  Zodiac  \n",
       "0                 4              3      10  \n",
       "1                 6              1      11  \n",
       "2                 4              8       2  \n",
       "3                 3              6      11  \n",
       "4                 5              4       8  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# from plot_lib import set_default, plot_data, plot_model, set_default\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if len(data[column].unique())>2:\n",
    "        # create an instance of MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        # fit the scaler to the data and transform the data\n",
    "        data_scaled = scaler.fit_transform(np.array(data[column]).reshape(-1, 1))\n",
    "        # print the scaled data\n",
    "        data[column] = data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Diabetes',axis=1).to_numpy()\n",
    "y = data['Diabetes'].to_numpy()\n",
    "# X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build and train a Perceptron (one input layer, one output layer, no hidden layers and no activation functions) to classify diabetes from the rest of the dataset. What is the AUC of this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron:\n",
    "1. To answer the question, we built and trained a Perceptron model for classifying diabetes from the given dataset. The Perceptron model consists of one input layer and one output layer without any hidden layers or activation functions. The dataset was first split into training and testing sets using a 70/30 ratio. We then defined the Perceptron model and set hyperparameters such as the number of epochs and learning rate. The model was trained using Stochastic Gradient Descent (SGD) as the optimizer and Mean Squared Error (MSE) loss as the loss function. Finally, we made predictions on the test set and computed the Area Under the Receiver Operating Characteristic (ROC) Curve (AUC) score to evaluate the model.\n",
    "\n",
    "2. We chose to build a Perceptron model because it is a simple and straightforward approach to binary classification problems like diabetes classification. A Perceptron can be used as a baseline model to compare more complex models against. Specific design choices, such as using MSE loss, were made because it is a common loss function for regression tasks and can be applied here since we are not using any activation function in the output layer. The learning rate and number of epochs were chosen based on typical values for training neural networks. The SGD optimizer was selected for its simplicity and effectiveness in training shallow models like a Perceptron.\n",
    "\n",
    "3. After training the Perceptron model and making predictions on the test set, we found an AUC score of 0.7024552688389927. This value quantifies the model's ability to differentiate between positive (diabetes) and negative (non-diabetes) cases.\n",
    "\n",
    "4. The obtained AUC score indicates that the Perceptron model has a moderate ability to classify diabetes cases from the given dataset. Although the model is relatively simple and may not capture complex relationships within the data, it still provides a reasonable baseline for comparing more sophisticated models. It's important to note that the performance of the model may be improved by tuning hyperparameters, using more advanced optimization techniques, or employing more complex neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "model = Perceptron(input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.1814\n",
      "Epoch [20/100], Loss: 0.1640\n",
      "Epoch [30/100], Loss: 0.1554\n",
      "Epoch [40/100], Loss: 0.1482\n",
      "Epoch [50/100], Loss: 0.1421\n",
      "Epoch [60/100], Loss: 0.1370\n",
      "Epoch [70/100], Loss: 0.1326\n",
      "Epoch [80/100], Loss: 0.1288\n",
      "Epoch [90/100], Loss: 0.1257\n",
      "Epoch [100/100], Loss: 0.1229\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.562995391339436\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred_probs = torch.sigmoid(y_pred)\n",
    "\n",
    "# Compute the AUC score\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(\"AUC Score:\", auc_score)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build and train a feedforward neural network with at least one hidden layer to classify diabetes from the rest of the dataset. Make sure to try different numbers of hidden layers and different activation functions (at a minimum reLU and sigmoid). Doing so: How does AUC vary as a function of the number of hidden layers and is it dependent on the kind of activation function used (make sure to include “no activation function” in your comparison). How does this network perform relative to the Perceptron?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Network\n",
    "1. To answer the question, we built and trained a feedforward neural network with at least one hidden layer to classify diabetes from the given dataset. We experimented with different numbers of hidden layers (1 to 3) and different activation functions (none, ReLU, and Sigmoid) in the hidden layers. The model architecture was designed using a custom class `FeedForwardNN` that allowed us to easily change the number of hidden layers and activation functions. We trained the model using the Adam optimizer and the Mean Squared Error (MSE) loss function. After training, we evaluated the model's performance on the test set and calculated the AUC score for each combination of hidden layers and activation functions.\n",
    "\n",
    "2. We chose to build a feedforward neural network with various numbers of hidden layers and activation functions to explore the impact of these factors on the model's performance. The feedforward neural network is a more powerful model than the Perceptron, as it can capture more complex relationships in the data. We used the custom class `FeedForwardNN` to easily experiment with different model configurations. The MSE loss function was chosen for consistency with the Perceptron, and the Adam optimizer was selected for its ability to efficiently train deep neural networks.\n",
    "\n",
    "3. The results from the experiment are as follows:\n",
    "\n",
    "- Hidden Layers: 1, Activation Function: none, AUC Score: 0.8153\n",
    "- Hidden Layers: 1, Activation Function: relu, AUC Score: 0.8162\n",
    "- Hidden Layers: 1, Activation Function: sigmoid, AUC Score: 0.7796\n",
    "- Hidden Layers: 2, Activation Function: none, AUC Score: 0.8171\n",
    "- Hidden Layers: 2, Activation Function: relu, AUC Score: 0.8204\n",
    "- Hidden Layers: 2, Activation Function: sigmoid, AUC Score: 0.7824\n",
    "- Hidden Layers: 3, Activation Function: none, AUC Score: 0.8126\n",
    "- Hidden Layers: 3, Activation Function: relu, AUC Score: 0.8179\n",
    "- Hidden Layers: 3, Activation Function: sigmoid, AUC Score: 0.7753\n",
    "\n",
    "4. The findings indicate that the feedforward neural network performs better than the Perceptron model, as the AUC scores are higher for all combinations of hidden layers and activation functions. The number of hidden layers and the choice of activation function do have an impact on the performance, with 2 hidden layers and ReLU activation yielding the best AUC score of 0.8204. The results suggest that using more complex models like feedforward neural networks with appropriate hidden layer and activation function choices can lead to better classification performance compared to simpler models like the Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers, activation_function):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, hidden_size)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers) - 1: # Don't apply activation to the output layer\n",
    "                if self.activation_function == 'relu':\n",
    "                    x = nn.ReLU()(x)\n",
    "                elif self.activation_function == 'sigmoid':\n",
    "                    x = nn.Sigmoid()(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, epochs, learning_rate):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        y_pred_probs = torch.sigmoid(y_pred)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "    return auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layers: 1, Activation Function: none, AUC Score: 0.8153\n",
      "Hidden Layers: 1, Activation Function: relu, AUC Score: 0.8162\n",
      "Hidden Layers: 1, Activation Function: sigmoid, AUC Score: 0.7796\n",
      "Hidden Layers: 2, Activation Function: none, AUC Score: 0.8171\n",
      "Hidden Layers: 2, Activation Function: relu, AUC Score: 0.8204\n",
      "Hidden Layers: 2, Activation Function: sigmoid, AUC Score: 0.7824\n",
      "Hidden Layers: 3, Activation Function: none, AUC Score: 0.8126\n",
      "Hidden Layers: 3, Activation Function: relu, AUC Score: 0.8179\n",
      "Hidden Layers: 3, Activation Function: sigmoid, AUC Score: 0.7753\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "combinations = [(num_hidden_layers, activation_function) for num_hidden_layers in range(1, 4) for activation_function in ['none', 'relu', 'sigmoid']]\n",
    "\n",
    "for num_hidden_layers, activation_function in combinations:\n",
    "    model = FeedForwardNN(input_size, hidden_size, num_hidden_layers, activation_function)\n",
    "    trained_model = train_model(model, X_train, y_train, epochs, learning_rate)\n",
    "    auc_score = evaluate_model(trained_model, X_test, y_test)\n",
    "    \n",
    "    print(f\"Hidden Layers: {num_hidden_layers}, Activation Function: {activation_function}, AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build and train a “deep” network (at least 2 hidden layers) to classify diabetes from the rest of the dataset. Given the nature of this dataset, is there a benefit of using a CNN or RNN for the classification?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The deep network\n",
    "\n",
    "Given the nature of this dataset, there is no significant benefit of using a CNN or RNN for the classification. The dataset consists of structured tabular data, and CNNs are typically more suited for image-based data, while RNNs are more appropriate for sequence-based data, such as time series or natural language. A feedforward neural network is generally better suited for this type of structured data.\n",
    "\n",
    "1. To answer the question, we built and trained a deep feedforward neural network with two hidden layers to classify diabetes from the given dataset. We implemented a custom class `DeepFeedForwardNN` to define the deep neural network model with ReLU activation functions in the hidden layers. We trained the model using the Adam optimizer and the Mean Squared Error (MSE) loss function. After training, we evaluated the model's performance on the test set and calculated the AUC score.\n",
    "\n",
    "2. We chose to build a deep feedforward neural network with two hidden layers to explore the potential benefits of using a deeper architecture for this dataset. The deep feedforward neural network can capture more complex relationships in the data, which might improve classification performance. We used the custom class `DeepFeedForwardNN` to implement the network and trained it using the same optimizer and loss function as the previous experiments for consistency.\n",
    "\n",
    "3. The result from the experiment is as follows:\n",
    "\n",
    "- AUC Score: 0.8065\n",
    "\n",
    "4. The findings indicate that the deep feedforward neural network with two hidden layers performs better than the Perceptron model but slightly worse than the best feedforward neural network from the previous experiment (2 hidden layers and ReLU activation with an AUC score of 0.8204). This suggests that while deeper networks can capture more complex relationships, they might not always provide significant improvements over shallower networks for structured tabular data. The choice of model architecture and complexity should be tailored to the dataset and the problem being solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super(DeepFeedForwardNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC Score: 0.8065\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# combinations = [(num_hidden_layers, activation_function) for num_hidden_layers in range(1, 4) for activation_function in ['none', 'relu', 'sigmoid']]\n",
    "\n",
    "\n",
    "model = DeepFeedForwardNN(input_size, hidden_size,32)\n",
    "trained_model = train_model(model, X_train, y_train, epochs, learning_rate)\n",
    "auc_score = evaluate_model(trained_model, X_test, y_test)\n",
    "\n",
    "print(f\" AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build and train a feedforward neural network with one hidden layer to predict BMI from the rest of the dataset. Use RMSE to assess the accuracy of your model. Does the RMSE depend on the activation function used?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting BMI\n",
    "\n",
    "To investigate whether the RMSE depends on the activation function used, we built and trained a feedforward neural network with one hidden layer to predict BMI from the rest of the dataset. We implemented a custom class `FeedForwardNN` to define the neural network model and experimented with different activation functions (ReLU, Sigmoid, Tanh, and none). We trained the model using the Adam optimizer and the Mean Squared Error (MSE) loss function. After training, we evaluated the model's performance on the test set and calculated the RMSE.\n",
    "\n",
    "The rationale for building a feedforward neural network with one hidden layer is that it is a simple yet powerful model capable of handling structured data like the dataset in question. We chose different activation functions to explore their impact on the model's performance. Using the custom class `FeedForwardNN`, we were able to implement the network and train it with the same optimizer and loss function as the previous experiments for consistency.\n",
    "\n",
    "The results from the experiment are as follows:\n",
    "\n",
    "- Activation Function: none, RMSE: 8.9167\n",
    "- Activation Function: ReLU, RMSE: 9.8926\n",
    "- Activation Function: Sigmoid, RMSE: 28.1875\n",
    "- Activation Function: Tanh, RMSE: 28.1727\n",
    "\n",
    "The findings suggest that the choice of activation function does have an impact on the RMSE of the model. Specifically, the model with no activation function and the model with ReLU activation function perform considerably better than the models with Sigmoid and Tanh activation functions. This indicates that for this specific dataset and problem, using no activation function or ReLU activation function in the hidden layer of the feedforward neural network leads to better performance in terms of predicting BMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"BMI\", axis=1).to_numpy()\n",
    "y = data[\"BMI\"].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, activation_function):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if self.activation_function == 'relu':\n",
    "            x = nn.ReLU()(x)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            x = nn.Sigmoid()(x)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            x = nn.Tanh()(x)\n",
    "        x = self.fc2(x)\n",
    "        if self.activation_function == 'relu':\n",
    "            x = nn.ReLU()(x)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            x = nn.Sigmoid()(x)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            x = nn.Tanh()(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, epochs, learning_rate):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        rmse = torch.sqrt(nn.MSELoss()(y_pred, y_test))\n",
    "    return rmse.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Function: none, RMSE: 8.9167\n",
      "Activation Function: relu, RMSE: 9.8926\n",
      "Activation Function: sigmoid, RMSE: 28.1875\n",
      "Activation Function: tanh, RMSE: 28.1727\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "activation_functions = ['none', 'relu', 'sigmoid', 'tanh']\n",
    "\n",
    "for activation_function in activation_functions:\n",
    "    model = FeedForwardNN(input_size, hidden_size, activation_function)\n",
    "    trained_model = train_model(model, X_train, y_train, epochs, learning_rate)\n",
    "    rmse = evaluate_model(trained_model, X_test, y_test)\n",
    "    \n",
    "    print(f\"Activation Function: {activation_function}, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build and train a neural network of your choice to predict BMI from the rest of your dataset. How low can you get RMSE and what design choices does RMSE seem to depend on?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The experiment\n",
    "To answer the question, we designed and trained a neural network with a custom architecture to predict BMI using the dataset. We first modified the train_model function to include different optimizers (Adam and SGD). We then performed a grid search over hyperparameters using k-fold cross-validation, considering different combinations of hidden layer sizes, activation functions, learning rates, and optimizers. For each combination, we built a neural network model using the specified hyperparameters, trained the model, and evaluated the model on the validation set to compute the average RMSE.\n",
    "We performed a grid search over various hyperparameters to find the best possible model. This approach was chosen because it allows for a systematic exploration of the hyperparameter space and helps identify the combination that results in the lowest RMSE. The grid search considered multiple hidden layer sizes, activation functions, learning rates, and optimizers to find the best neural network architecture and training parameters for predicting BMI. By using k-fold cross-validation, we ensured a robust evaluation of the model's performance on unseen data.\n",
    "The best model achieved an RMSE of 6.7677, with the following hyperparameters: hidden layer sizes of (128, 64), activation function 'ReLU', learning rate 0.01, and optimizer 'Adam'. The grid search output shows the average RMSE for different combinations of hyperparameters, indicating the model's performance for each configuration. Some configurations resulted in a higher RMSE, while others achieved a lower RMSE, depending on the chosen hyperparameters.\n",
    "The findings suggest that the RMSE depends on the chosen hyperparameters, and it is possible to achieve a lower RMSE by selecting the right combination of hidden layer sizes, activation function, learning rate, and optimizer. The best model, with an RMSE of 6.7677, outperforms the previous models built in this project. This demonstrates that exploring different neural network architectures and training parameters can lead to improved prediction performance. However, it is worth noting that further tuning of hyperparameters or considering additional regularization techniques could potentially improve the performance even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, epochs, learning_rate, optimizer_name):\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer: {optimizer_name}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Sizes: (64,), Activation: relu, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 7.1510\n",
      "Hidden Sizes: (64,), Activation: relu, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (64,), Activation: relu, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 8.8554\n",
      "Hidden Sizes: (64,), Activation: relu, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 7.2818\n",
      "Hidden Sizes: (64,), Activation: relu, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 27.0579\n",
      "Hidden Sizes: (64,), Activation: relu, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.3386\n",
      "Hidden Sizes: (64,), Activation: sigmoid, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 7.1794\n",
      "Hidden Sizes: (64,), Activation: sigmoid, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (64,), Activation: sigmoid, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 8.8338\n",
      "Hidden Sizes: (64,), Activation: sigmoid, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 7.2798\n",
      "Hidden Sizes: (64,), Activation: sigmoid, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 26.7269\n",
      "Hidden Sizes: (64,), Activation: sigmoid, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.3134\n",
      "Hidden Sizes: (64,), Activation: tanh, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 7.1629\n",
      "Hidden Sizes: (64,), Activation: tanh, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (64,), Activation: tanh, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 8.8375\n",
      "Hidden Sizes: (64,), Activation: tanh, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 7.2769\n",
      "Hidden Sizes: (64,), Activation: tanh, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 26.3176\n",
      "Hidden Sizes: (64,), Activation: tanh, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.3634\n",
      "Hidden Sizes: (128,), Activation: relu, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.9498\n",
      "Hidden Sizes: (128,), Activation: relu, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (128,), Activation: relu, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 8.0572\n",
      "Hidden Sizes: (128,), Activation: relu, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 7.2036\n",
      "Hidden Sizes: (128,), Activation: relu, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 24.2621\n",
      "Hidden Sizes: (128,), Activation: relu, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.1854\n",
      "Hidden Sizes: (128,), Activation: sigmoid, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.9582\n",
      "Hidden Sizes: (128,), Activation: sigmoid, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (128,), Activation: sigmoid, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 7.9412\n",
      "Hidden Sizes: (128,), Activation: sigmoid, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 7.2506\n",
      "Hidden Sizes: (128,), Activation: sigmoid, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 24.0628\n",
      "Hidden Sizes: (128,), Activation: sigmoid, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.2304\n",
      "Hidden Sizes: (128,), Activation: tanh, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.9798\n",
      "Hidden Sizes: (128,), Activation: tanh, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (128,), Activation: tanh, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 8.0488\n",
      "Hidden Sizes: (128,), Activation: tanh, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 7.2097\n",
      "Hidden Sizes: (128,), Activation: tanh, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 25.4474\n",
      "Hidden Sizes: (128,), Activation: tanh, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.2290\n",
      "Hidden Sizes: (64, 32), Activation: relu, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.8767\n",
      "Hidden Sizes: (64, 32), Activation: relu, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (64, 32), Activation: relu, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 7.9135\n",
      "Hidden Sizes: (64, 32), Activation: relu, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 12.5065\n",
      "Hidden Sizes: (64, 32), Activation: relu, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 25.3316\n",
      "Hidden Sizes: (64, 32), Activation: relu, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.2952\n",
      "Hidden Sizes: (64, 32), Activation: sigmoid, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.8461\n",
      "Hidden Sizes: (64, 32), Activation: sigmoid, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (64, 32), Activation: sigmoid, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 7.9459\n",
      "Hidden Sizes: (64, 32), Activation: sigmoid, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 11.1590\n",
      "Hidden Sizes: (64, 32), Activation: sigmoid, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 24.2318\n",
      "Hidden Sizes: (64, 32), Activation: sigmoid, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.3026\n",
      "Hidden Sizes: (64, 32), Activation: tanh, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.8251\n",
      "Hidden Sizes: (64, 32), Activation: tanh, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (64, 32), Activation: tanh, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 7.9691\n",
      "Hidden Sizes: (64, 32), Activation: tanh, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 9.0565\n",
      "Hidden Sizes: (64, 32), Activation: tanh, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 25.0314\n",
      "Hidden Sizes: (64, 32), Activation: tanh, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.2954\n",
      "Hidden Sizes: (128, 64), Activation: relu, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.7677\n",
      "Hidden Sizes: (128, 64), Activation: relu, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (128, 64), Activation: relu, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 7.8134\n",
      "Hidden Sizes: (128, 64), Activation: relu, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 10.1651\n",
      "Hidden Sizes: (128, 64), Activation: relu, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 18.9187\n",
      "Hidden Sizes: (128, 64), Activation: relu, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.1687\n",
      "Hidden Sizes: (128, 64), Activation: sigmoid, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.7833\n",
      "Hidden Sizes: (128, 64), Activation: sigmoid, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (128, 64), Activation: sigmoid, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 7.7103\n",
      "Hidden Sizes: (128, 64), Activation: sigmoid, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 13.1444\n",
      "Hidden Sizes: (128, 64), Activation: sigmoid, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 18.4226\n",
      "Hidden Sizes: (128, 64), Activation: sigmoid, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.1826\n",
      "Hidden Sizes: (128, 64), Activation: tanh, Learning Rate: 0.01, Optimizer: adam, Avg RMSE: 6.7882\n",
      "Hidden Sizes: (128, 64), Activation: tanh, Learning Rate: 0.01, Optimizer: sgd, Avg RMSE: nan\n",
      "Hidden Sizes: (128, 64), Activation: tanh, Learning Rate: 0.001, Optimizer: adam, Avg RMSE: 7.6195\n",
      "Hidden Sizes: (128, 64), Activation: tanh, Learning Rate: 0.001, Optimizer: sgd, Avg RMSE: 12.5464\n",
      "Hidden Sizes: (128, 64), Activation: tanh, Learning Rate: 0.0001, Optimizer: adam, Avg RMSE: 18.6193\n",
      "Hidden Sizes: (128, 64), Activation: tanh, Learning Rate: 0.0001, Optimizer: sgd, Avg RMSE: 8.2123\n",
      "Best RMSE: 6.7677 with parameters: ((128, 64), 'relu', 0.01, 'adam')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# Define hyperparameter search space\n",
    "hidden_layer_sizes = [(64,), (128,), (64, 32), (128, 64)]\n",
    "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "optimizers = ['adam', 'sgd']\n",
    "\n",
    "# Perform a grid search over hyperparameters\n",
    "best_rmse = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for hidden_sizes, activation_function, learning_rate, optimizer in product(hidden_layer_sizes, activation_functions, learning_rates, optimizers):\n",
    "    # Perform k-fold cross-validation\n",
    "    kfold = KFold(n_splits=5)\n",
    "    rmse_sum = 0\n",
    "\n",
    "    for train_index, val_index in kfold.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # Build the model\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0])]\n",
    "        layers.extend([nn.Linear(hs1, hs2) for hs1, hs2 in zip(hidden_sizes, hidden_sizes[1:])])\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        model = nn.Sequential(*layers)\n",
    "\n",
    "        # Train the model\n",
    "        trained_model = train_model(model, X_train_fold, y_train_fold, epochs, learning_rate, optimizer)\n",
    "\n",
    "        # Evaluate the model\n",
    "        rmse = evaluate_model(trained_model, X_val_fold, y_val_fold)\n",
    "        rmse_sum += rmse\n",
    "\n",
    "    # Calculate average RMSE for the current set of hyperparameters\n",
    "    avg_rmse = rmse_sum / kfold.get_n_splits()\n",
    "\n",
    "    if avg_rmse < best_rmse:\n",
    "        best_rmse = avg_rmse\n",
    "        best_params = (hidden_sizes, activation_function, learning_rate, optimizer)\n",
    "\n",
    "    print(f\"Hidden Sizes: {hidden_sizes}, Activation: {activation_function}, Learning Rate: {learning_rate}, Optimizer: {optimizer}, Avg RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "print(f\"Best RMSE: {best_rmse:.4f} with parameters: {best_params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Extra credit:\n",
    "a) Are there any predictors/features that have effectively no impact on the accuracy of these\n",
    "models? If so, please list them and comment briefly on your findings\n",
    "\n",
    "b) Write a summary statement on the overall pros and cons of using neural networks to learn from the same dataset as in the prior homework, relative to using classical methods\n",
    "(logistic regression, SVM, trees, forests, boosting methods). Any overall lessons?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
